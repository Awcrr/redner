{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 2 meshes in the session\n",
      "Forward pass, time: 0.07200 s\n",
      "Scene construction, time: 0.11887 s\n",
      "Forward pass, time: 0.11798 s\n",
      "Detach\n",
      "Detach\n",
      "Detach\n",
      "Detach\n",
      "Detach\n",
      "Detach\n",
      "Detach\n",
      "Detach\n",
      "Detach\n",
      "Detach\n",
      "Detach\n",
      "Forward pass, time: 0.07692 s\n",
      "Scene construction, time: 0.16078 s\n",
      "Forward pass, time: 0.12383 s\n",
      "Detach\n",
      "Detach\n",
      "Detach\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "tfe = tf.contrib.eager\n",
    "import numpy as np\n",
    "import skimage.transform\n",
    "import pdb\n",
    "import pyrednertensorflow as pyredner\n",
    "import redner\n",
    "\n",
    "import pyrednertensorflow as pyrednertorch\n",
    "import torch\n",
    "\n",
    "import utils \n",
    "\n",
    "# Set up the pyredner scene for rendering:\n",
    "material_map, mesh_list, light_map = pyredner.load_obj('scenes/teapot.obj')\n",
    "for _, mesh in mesh_list:\n",
    "    # mesh.normals = pyredner.pyredner.compute_vertex_normal(mesh.vertices, mesh.indices)\n",
    "    mesh.normals = pyredner.compute_vertex_normal(mesh.vertices, mesh.indices)\n",
    "\n",
    "material_maptorch, mesh_listtorch, light_maptorch = pyrednertorch.load_obj('scenes/teapot.obj')\n",
    "for _, mesh in mesh_listtorch:\n",
    "    mesh.normals = pyrednertorch.compute_vertex_normal(mesh.vertices, mesh.indices)\n",
    "\n",
    "for (_, mesh), (_, meshtf) in zip(mesh_list, mesh_listtorch):\n",
    "    assert utils.is_same_tensor(mesh.normals, meshtf.normals)\n",
    "\n",
    "# Setup camera\n",
    "cam = pyredner.Camera(position = tfe.Variable([0.0, 30.0, 200.0]),\n",
    "                      look_at = tfe.Variable([0.0, 30.0, 0.0]),\n",
    "                      up = tfe.Variable([0.0, 1.0, 0.0], dtype=tf.float32),\n",
    "                      fov = tfe.Variable([45.0], dtype=tf.float32), # in degree\n",
    "                      clip_near = 1e-2, # needs to > 0\n",
    "                      resolution = (256, 256),\n",
    "                      fisheye = False)\n",
    "\n",
    "\n",
    "# Setup camera\n",
    "camtorch = pyrednertorch.Camera(position = torch.tensor([0.0, 30.0, 200.0]),\n",
    "                      look_at = torch.tensor([0.0, 30.0, 0.0]),\n",
    "                      up = torch.tensor([0.0, 1.0, 0.0]),\n",
    "                      fov = torch.tensor([45.0]), # in degree\n",
    "                      clip_near = 1e-2, # needs to > 0\n",
    "                      resolution = (256, 256),\n",
    "                      fisheye = False)\n",
    "\n",
    "assert utils.is_same_camera(cam, camtorch)\n",
    "\n",
    "material_id_map = {}\n",
    "materials = []\n",
    "count = 0\n",
    "for key, value in material_map.items():\n",
    "    material_id_map[key] = count\n",
    "    count += 1\n",
    "    materials.append(value)\n",
    "\n",
    "material_id_maptorch = {}\n",
    "materialstorch = []\n",
    "count = 0\n",
    "for key, value in material_maptorch.items():\n",
    "    material_id_maptorch[key] = count\n",
    "    count += 1\n",
    "    materialstorch.append(value)\n",
    "\n",
    "assert material_id_map == material_id_maptorch\n",
    "assert utils.is_same_container(materials, materialstorch)\n",
    "\n",
    "shapes = []\n",
    "print(f\">>> {len(mesh_list)} meshes in the session\")\n",
    "for mtl_name, mesh in mesh_list:\n",
    "    shapes.append(pyredner.Shape(\n",
    "        vertices = mesh.vertices,\n",
    "        indices = mesh.indices,\n",
    "        uvs = mesh.uvs,\n",
    "        normals = mesh.normals,\n",
    "        material_id = material_id_map[mtl_name]))\n",
    "\n",
    "shapestorch = []\n",
    "for mtl_name, mesh in mesh_listtorch:\n",
    "    shapestorch.append(pyrednertorch.Shape(\\\n",
    "        vertices = mesh.vertices,\n",
    "        indices = mesh.indices,\n",
    "        uvs = mesh.uvs,\n",
    "        normals = mesh.normals,\n",
    "        material_id = material_id_maptorch[mtl_name]))\n",
    "\n",
    "assert utils.is_same_container(shapes, shapestorch)\n",
    "\n",
    "scene = pyredner.Scene(cam, shapes, materials, area_lights = [], envmap = None)\n",
    "scenetorch = pyrednertorch.Scene(camtorch, shapestorch, materialstorch, area_lights = [], envmap = None)\n",
    "\n",
    "assert utils.is_same_scene(scene, scenetorch)\n",
    "\n",
    "scene_args = pyredner.RenderFunction.serialize_scene(\n",
    "    scene = scene,\n",
    "    num_samples = 16,\n",
    "    max_bounces = 0,\n",
    "    channels = [redner.channels.depth, redner.channels.shading_normal])\n",
    "\n",
    "scene_argstorch = pyrednertorch.RenderFunction.serialize_scene(\\\n",
    "    scene = scenetorch,\n",
    "    num_samples = 16,\n",
    "    max_bounces = 0,\n",
    "    channels = [redner.channels.depth, redner.channels.shading_normal])\n",
    "\n",
    "assert utils.is_same_scene_args(scene_args, scene_argstorch)\n",
    "\n",
    "render = pyredner.RenderFunction\n",
    "# Render. The first argument is the seed for RNG in the renderer.\n",
    "img = render.forward(0, scene_args)\n",
    "\n",
    "rendertorch = pyrednertorch.RenderFunction.apply\n",
    "# Render. The first argument is the seed for RNG in the renderer.\n",
    "imgtorch = rendertorch(0, *scene_argstorch)\n",
    "\n",
    "assert utils.is_same_tensor(img, imgtorch)\n",
    "\n",
    "depth = img[:, :, 0]\n",
    "normal = img[:, :, 1:4]\n",
    "depthtorch = imgtorch[:, :, 0]\n",
    "normaltorch = imgtorch[:, :, 1:4]\n",
    "\n",
    "target_depth = pyredner.imread('results/test_g_buffer/target_depth.exr')\n",
    "target_depth = target_depth[:, :, 0]\n",
    "target_normal = pyredner.imread('results/test_g_buffer/target_normal.exr')\n",
    "\n",
    "target_depthtorch = pyrednertorch.imread('results/test_g_buffer/target_depth.exr')\n",
    "target_depthtorch = target_depthtorch[:, :, 0]\n",
    "target_normaltorch = pyrednertorch.imread('results/test_g_buffer/target_normal.exr')\n",
    "\n",
    "assert utils.is_same_tensor(target_depth, target_depthtorch)\n",
    "assert utils.is_same_tensor(target_normal, target_normaltorch)\n",
    "\n",
    "translation_params = tfe.Variable([0.1, -0.1, 0.1], trainable=True)\n",
    "translation = translation_params * 100.0\n",
    "euler_angles = tfe.Variable([0.1, -0.1, 0.1], trainable=True)\n",
    "\n",
    "translation_paramstorch = torch.tensor([0.1, -0.1, 0.1],\n",
    "    device = pyrednertorch.get_device(), requires_grad=True)\n",
    "translationtorch = translation_paramstorch * 100.0\n",
    "euler_anglestorch = torch.tensor([0.1, -0.1, 0.1], requires_grad=True)\n",
    "\n",
    "assert utils.is_same_tensor(translation, translationtorch)\n",
    "assert utils.is_same_tensor(euler_angles, euler_anglestorch)\n",
    "\n",
    "shape0_vertices = tf.identity(shapes[0].vertices)\n",
    "shape1_vertices = tf.identity(shapes[1].vertices)\n",
    "\n",
    "shape0_verticestorch = shapestorch[0].vertices.clone()\n",
    "shape1_verticestorch = shapestorch[1].vertices.clone()\n",
    "\n",
    "assert utils.is_same_tensor(shape0_vertices, shape0_verticestorch)\n",
    "assert utils.is_same_tensor(shape1_vertices, shape1_verticestorch)\n",
    "\n",
    "rotation_matrix = pyredner.gen_rotate_matrix(euler_angles)\n",
    "rotation_matrixtorch = pyrednertorch.gen_rotate_matrix(euler_anglestorch)\n",
    "\n",
    "assert utils.is_same_tensor(rotation_matrix, rotation_matrixtorch)\n",
    "\n",
    "center = tf.math.reduce_mean(\n",
    "    tf.concat([shape0_vertices, shape1_vertices], axis=0), \n",
    "    axis=0\n",
    "    )\n",
    "centertorch = torch.mean(torch.cat([shape0_verticestorch, shape1_verticestorch]), 0)\n",
    "\n",
    "assert utils.is_same_tensor(center, centertorch)\n",
    "\n",
    "shapes[0].vertices = \\\n",
    "    (shape0_vertices - center) @ tf.transpose(rotation_matrix) + \\\n",
    "    center + translation\n",
    "shapes[1].vertices = \\\n",
    "    (shape1_vertices - center) @ tf.transpose(rotation_matrix) + \\\n",
    "    center + translation\n",
    "\n",
    "shapestorch[0].vertices = \\\n",
    "    (shape0_verticestorch - centertorch) @ torch.t(rotation_matrixtorch) + \\\n",
    "    centertorch + translationtorch\n",
    "shapestorch[1].vertices = \\\n",
    "    (shape1_verticestorch - centertorch) @ torch.t(rotation_matrixtorch) + \\\n",
    "    centertorch + translationtorch\n",
    "\n",
    "assert utils.is_same_tensor(shapes[0].vertices, shapestorch[0].vertices)\n",
    "assert utils.is_same_tensor(shapes[1].vertices, shapestorch[1].vertices)\n",
    "\n",
    "shapes[0].normals = pyredner.compute_vertex_normal(shapes[0].vertices, shapes[0].indices)\n",
    "shapes[1].normals = pyredner.compute_vertex_normal(shapes[1].vertices, shapes[1].indices)\n",
    "\n",
    "shapestorch[0].normals = pyrednertorch.compute_vertex_normal(shapestorch[0].vertices, shapestorch[0].indices)\n",
    "shapestorch[1].normals = pyrednertorch.compute_vertex_normal(shapestorch[1].vertices, shapestorch[1].indices)\n",
    "\n",
    "assert utils.is_same_tensor(shapes[0].normals, shapestorch[0].normals)\n",
    "assert utils.is_same_tensor(shapes[1].normals, shapestorch[1].normals)\n",
    "\n",
    "scene_args = pyredner.RenderFunction.serialize_scene(\n",
    "    scene = scene,\n",
    "    num_samples = 16,\n",
    "    max_bounces = 0,\n",
    "    channels = [redner.channels.depth, redner.channels.shading_normal])\n",
    "\n",
    "scene_argstorch = pyrednertorch.RenderFunction.serialize_scene(\\\n",
    "    scene = scenetorch,\n",
    "    num_samples = 16,\n",
    "    max_bounces = 0,\n",
    "    channels = [redner.channels.depth, redner.channels.shading_normal])\n",
    "\n",
    "assert utils.is_same_scene_args(scene_args, scene_argstorch)\n",
    "\n",
    "img = render.forward(1, scene_args)\n",
    "depth = img[:, :, 0]\n",
    "normal = img[:, :, 1:4]\n",
    "\n",
    "imgtorch = rendertorch(1, *scene_argstorch)\n",
    "depthtorch = imgtorch[:, :, 0]\n",
    "normaltorch = imgtorch[:, :, 1:4]\n",
    "\n",
    "assert utils.is_same_tensor(img, imgtorch)\n",
    "\n",
    "diff_depth = tf.abs(target_depth - depth)\n",
    "diff_normal = tf.abs(target_normal - normal)\n",
    "\n",
    "diff_depthtorch = torch.abs(target_depthtorch - depthtorch)\n",
    "diff_normaltorch = torch.abs(target_normaltorch - normaltorch)\n",
    "\n",
    "assert utils.is_same_tensor(diff_depth, diff_depthtorch)\n",
    "assert utils.is_same_tensor(diff_normal, diff_normaltorch)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(1e-2)\n",
    "optimizertorch = torch.optim.Adam([translation_paramstorch, euler_anglestorch], lr=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(6460), Dimension(3)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapes[0].vertices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detach\n",
      "Detach\n",
      "Detach\n",
      "Detach\n",
      "Detach\n",
      "Detach\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0704 14:16:58.918381 139977715046144 util.py:64] Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "W0704 14:16:58.948640 139977715046144 util.py:64] Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass, time: 0.06180 s\n",
      "loss: tf.Tensor(1588092.6, shape=(), dtype=float32)\n",
      "Backward pass, time: 1.13145 s\n",
      "Scene construction, time: 0.10722 s\n",
      "Forward pass, time: 0.08823 s\n",
      "Backward pass, time: 1.23783 s\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as g1:\n",
    "    g1.watch(translation_params)\n",
    "    translation = translation_params * 100.0\n",
    "    with tf.GradientTape(persistent=True) as g2:\n",
    "        g2.watch(euler_angles)\n",
    "        rotation_matrix = pyredner.gen_rotate_matrix(euler_angles)\n",
    "        center = tf.math.reduce_mean(\n",
    "            tf.concat([shape0_vertices, shape1_vertices], axis=0), \n",
    "            axis=0)\n",
    "        shapes[0].vertices = \\\n",
    "            (shape0_vertices - center) @ tf.transpose(rotation_matrix) + \\\n",
    "            center + translation\n",
    "        shapes[1].vertices = \\\n",
    "            (shape1_vertices - center) @ tf.transpose(rotation_matrix) + \\\n",
    "            center + translation\n",
    "\n",
    "d_translation_params_dshape0vertices = g1.gradient(shapes[0].vertices, translation_params)\n",
    "d_translation_params_dshape1vertices = g1.gradient(shapes[1].vertices, translation_params)\n",
    "\n",
    "deuler_dshape0vertices = g2.gradient(shapes[0].vertices, euler_angles)\n",
    "deuler_dshape1vertices = g2.gradient(shapes[1].vertices, euler_angles)\n",
    "\n",
    "shapes[0].normals = pyredner.compute_vertex_normal(shapes[0].vertices, shapes[0].indices)\n",
    "shapes[1].normals = pyredner.compute_vertex_normal(shapes[1].vertices, shapes[1].indices)\n",
    "del g1, g2\n",
    "\n",
    "\n",
    "optimizertorch.zero_grad()\n",
    "translationtorch = translation_paramstorch * 100.0\n",
    "rotation_matrixtorch = pyrednertorch.gen_rotate_matrix(euler_anglestorch)\n",
    "centertorch = torch.mean(torch.cat([shape0_verticestorch, shape1_verticestorch]), 0)\n",
    "shapestorch[0].vertices = \\\n",
    "    (shape0_verticestorch - centertorch) @ torch.t(rotation_matrixtorch) + \\\n",
    "    centertorch + translationtorch\n",
    "shapestorch[1].vertices = \\\n",
    "    (shape1_verticestorch - centertorch) @ torch.t(rotation_matrixtorch) + \\\n",
    "    centertorch + translationtorch\n",
    "\n",
    "shapestorch[0].normals = pyrednertorch.compute_vertex_normal(shapestorch[0].vertices, shapestorch[0].indices)\n",
    "shapestorch[1].normals = pyrednertorch.compute_vertex_normal(shapestorch[1].vertices, shapestorch[1].indices)\n",
    "\n",
    "\n",
    "assert utils.is_same_tensor(shapes[0].normals, shapestorch[0].normals)\n",
    "assert utils.is_same_tensor(shapes[1].normals, shapestorch[1].normals)\n",
    "\n",
    "\n",
    "scene_args = pyredner.RenderFunction.serialize_scene(\n",
    "    scene = scene,\n",
    "    num_samples = 16,\n",
    "    max_bounces = 0,\n",
    "    channels = [redner.channels.depth, redner.channels.shading_normal])\n",
    "\n",
    "scene_argstorch = pyrednertorch.RenderFunction.serialize_scene(\\\n",
    "    scene = scenetorch,\n",
    "    num_samples = 16,\n",
    "    max_bounces = 0,\n",
    "    channels = [redner.channels.depth, redner.channels.shading_normal])\n",
    "\n",
    "assert utils.is_same_scene_args(scene_args, scene_argstorch)\n",
    "\n",
    "img = render.forward(t+1, scene_args)\n",
    "\n",
    "with tf.GradientTape(persistent=False) as g3:\n",
    "    g3.watch(img)\n",
    "    depth = img[:, :, 0]\n",
    "    normal = img[:, :, 1:4]\n",
    "\n",
    "    # Save the intermediate render.\n",
    "    pyredner.imwrite(depth,\n",
    "        'results/test_g_buffer/iter_depth_{}.png'.format(t),\n",
    "        normalize = True)\n",
    "    pyredner.imwrite(normal,\n",
    "        'results/test_g_buffer/iter_normal_{}.png'.format(t),\n",
    "        normalize = True)\n",
    "    # Compute the loss function. Here it is L2.\n",
    "    loss = tf.reduce_sum(tf.square(depth - target_depth)) / 200.0 \\\n",
    "        + tf.reduce_sum(tf.square(normal - target_normal))\n",
    "    print('loss:', loss)\n",
    "\n",
    "d_img = g3.gradient(loss, img)\n",
    "grads = render.backward(d_img)\n",
    "\n",
    "##################################################################\n",
    "# Torch\n",
    "imgtorch = rendertorch(t+1, *scene_argstorch)\n",
    "depthtorch = imgtorch[:, :, 0]\n",
    "normaltorch = imgtorch[:, :, 1:4]\n",
    "losstorch = (depthtorch - target_depthtorch).pow(2).sum() / 200.0 + (normaltorch - target_normaltorch).pow(2).sum()\n",
    "losstorch.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(256), Dimension(256), Dimension(4)])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(256), Dimension(256), Dimension(4)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View grads from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1000, -0.1000,  0.1000], requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_paramstorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1892877.7500, -3885791.0000, -8072497.5000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td1 = translation_paramstorch.grad; td1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -916749.6875,    75257.5625, -1346990.6250])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de1 = euler_anglestorch.grad; de1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'd_vertices_0:0' shape=(6460, 3) dtype=float32, numpy=\n",
       "array([[-0.00082663,  0.00044479, -0.00012241],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads.d_vertices_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=5103, shape=(6460,), dtype=float32, numpy=\n",
       "array([-0.00050425,  0.        ,  0.        , ...,  0.        ,\n",
       "        0.        ,  0.        ], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(grads.d_vertices_list[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=5111, shape=(), dtype=float32, numpy=-87772.555>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(grads.d_vertices_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View grads from TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'd_position:0' shape=(3,) dtype=float32, numpy=array([-4303.9756,  6767.5195, 80900.06  ], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads.d_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(3)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_translation_params_dshape0vertices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=6280, shape=(6460, 3), dtype=float32, numpy=\n",
       "array([[41.045395 , 53.686386 ,  8.889906 ],\n",
       "       [41.378353 , 53.6588   ,  8.908552 ],\n",
       "       [41.26592  , 53.17974  ,  4.262364 ],\n",
       "       ...,\n",
       "       [81.39684  , 55.873413 , -3.1109543],\n",
       "       [83.08425  , 56.150414 , -3.173088 ],\n",
       "       [83.08425  , 56.150414 , -3.173088 ]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapes[0].vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'd_vertices_0:0' shape=(6460, 3) dtype=float32, numpy=\n",
       "array([[-0.00082663,  0.00044479, -0.00012241],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads.d_vertices_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=7013, shape=(6460,), dtype=float32, numpy=\n",
       "array([-0.00016808,  0.        ,  0.        , ...,  0.        ,\n",
       "        0.        ,  0.        ], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(grads.d_vertices_list[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=7021, shape=(), dtype=float32, numpy=-4.5290275>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(grads.d_vertices_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'd_vertices_1:0' shape=(1874, 3) dtype=float32, numpy=\n",
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads.d_vertices_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=3640, shape=(3,), dtype=float32, numpy=array([ 6.6481085e+09, -1.4127784e+10, -4.9221358e+10], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum or mean?\n",
    "tf.reduce_sum(d_translation_params_dshape0vertices * grads.d_vertices_list[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=3645, shape=(3,), dtype=float32, numpy=array([ 1.6186853e+09, -3.1835960e+09, -8.4908710e+08], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(d_translation_params_dshape1vertices * grads.d_vertices_list[1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=3689, shape=(6,), dtype=float32, numpy=\n",
       "array([ 6.6481085e+09, -1.4127784e+10, -4.9221358e+10,  1.6186853e+09,\n",
       "       -3.1835960e+09, -8.4908710e+08], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.concat([\n",
    "    tf.reduce_sum(d_translation_params_dshape0vertices * grads.d_vertices_list[0], axis=0),\n",
    "    tf.reduce_sum(d_translation_params_dshape1vertices * grads.d_vertices_list[1], axis=0)\n",
    "], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=3699, shape=(3,), dtype=float32, numpy=array([ 8.2667940e+09, -1.7311379e+10, -5.0070446e+10], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(d_translation_params_dshape0vertices * grads.d_vertices_list[0], axis=0) + tf.reduce_sum(d_translation_params_dshape1vertices * grads.d_vertices_list[1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "td2 = tf.reduce_mean(d_translation_params_dshape0vertices * grads.d_vertices_list[0], axis=0) + tf.reduce_mean(d_translation_params_dshape1vertices * grads.d_vertices_list[1], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare d_translation_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.75, -3.5 , -4.5 ], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td1.numpy() - td2.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare d_euler_angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=3749, shape=(3,), dtype=float32, numpy=array([-185311.  ,  -56637.47, -495857.06], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de2 = tf.reduce_mean(deuler_dshape0vertices * grads.d_vertices_list[0], axis=0) + tf.reduce_mean(deuler_dshape1vertices * grads.d_vertices_list[1], axis=0); de2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-731438.7 ,  131895.03, -851133.56], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de1.numpy() - de2.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(6460), Dimension(3)]),\n",
       " TensorShape([Dimension(1874), Dimension(3)]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapes[0].vertices.shape, shapes[1].vertices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1989, shape=(3,), dtype=float32, numpy=array([646000., 646000., 646000.], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_translation_params_dshape0vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(6460), Dimension(3)])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads.d_vertices_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0705 07:07:46.508325 139977715046144 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "W0705 07:07:46.509212 139977715046144 backprop.py:820] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "W0705 07:07:46.511441 139977715046144 backprop.py:954] The dtype of the target tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n",
      "W0705 07:07:46.512087 139977715046144 backprop.py:968] The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n",
      "W0705 07:07:46.512669 139977715046144 backprop.py:968] The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=7235, shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>, <tf.Tensor: id=7236, shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>]\n",
      "2\n",
      "[<tf.Tensor: id=7235, shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>, <tf.Tensor: id=7236, shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>]\n",
      "[<tf.Tensor: id=7235, shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>, <tf.Tensor: id=7236, shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>]\n",
      "2\n",
      "[<tf.Tensor: id=7235, shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>, <tf.Tensor: id=7236, shape=(1,), dtype=int32, numpy=array([8], dtype=int32)>]\n",
      "[None, None]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def render(scene_args):\n",
    "    print(len(scene_args))\n",
    "    print(scene_args)\n",
    "    if scene_args[0]:\n",
    "        return scene_args[1] * 2\n",
    "    else:\n",
    "        return scene_args[1] * 40\n",
    "\n",
    "def backward(d_img):\n",
    "    return 2\n",
    "    return 2 * d_img\n",
    "    \n",
    "@tf.custom_gradient\n",
    "def render_op(scene_args):\n",
    "    print(scene_args)\n",
    "    img = render(scene_args)\n",
    "    def grad(d_img):\n",
    "        return None, backward(d_img)\n",
    "    \n",
    "    return img, grad\n",
    "\n",
    "x = [\n",
    "    tf.constant([8]),\n",
    "    tf.constant([8])\n",
    "]\n",
    "\n",
    "# img, grad = render_op(x)\n",
    "img = render_op(x)\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    img = render_op(x)\n",
    "    \n",
    "print(g.gradient(img, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.custom_gradient\n",
    "def log1pexp(x):\n",
    "  e = tf.exp(x)\n",
    "  def grad(dy):\n",
    "    return dy * (1 - 1 / (1 + e))\n",
    "  return tf.math.log(1 + e), grad\n",
    "\n",
    "def grad_log1pexp(x):\n",
    "  with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    value = log1pexp(x)\n",
    "  return tape.gradient(value, x)\n",
    "\n",
    "\n",
    "# As before, the gradient computation works fine at x = 0.\n",
    "grad_log1pexp(tf.constant(0.)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=7408, shape=(), dtype=float32, numpy=0.8807971>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.custom_gradient\n",
    "def log1pexp(x):\n",
    "    e = tf.exp(x)\n",
    "    def grad(dy):\n",
    "        return dy * (1 - 1 / (1 + e))\n",
    "    return tf.math.log(1 + e), grad\n",
    "\n",
    "\n",
    "def get_grad(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        y = log1pexp(x)\n",
    "    \n",
    "    return tape.gradient(y, x)\n",
    "\n",
    "get_grad(tf.constant(2.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "1.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "('custom_gradient function expected to return', 2, 'gradients but returned', 1, 'instead.')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-3fafb66f68fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# As before, the gradient computation works fine at x = 0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mgrad_log1pexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-84-3fafb66f68fa>\u001b[0m in \u001b[0;36mgrad_log1pexp\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog1pexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py\u001b[0m in \u001b[0;36mactual_grad_fn\u001b[0;34m(*result_grads)\u001b[0m\n\u001b[1;32m    289\u001b[0m       raise ValueError(\n\u001b[1;32m    290\u001b[0m           \u001b[0;34m\"custom_gradient function expected to return\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m           \"gradients but returned\", len(flat_grads), \"instead.\")\n\u001b[0m\u001b[1;32m    292\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_grads\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvariable_grads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('custom_gradient function expected to return', 2, 'gradients but returned', 1, 'instead.')"
     ]
    }
   ],
   "source": [
    "@tf.custom_gradient\n",
    "def log1pexp(*x):\n",
    "    print(b)\n",
    "    x = tf.stack(x)\n",
    "    e = tf.exp(x)\n",
    "    def grad(dy):\n",
    "        print(dy.numpy())\n",
    "        return dy * (1 - 1 / (1 + e))\n",
    "    return tf.math.log(1 + e), grad\n",
    "\n",
    "def grad_log1pexp(*x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        y = log1pexp(*x)\n",
    "    \n",
    "    return y, tape.gradient(y, x)\n",
    "\n",
    "\n",
    "# As before, the gradient computation works fine at x = 0.\n",
    "grad_log1pexp(tf.constant(0.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "1.0\n",
      "10.0\n",
      "20.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(0.)\n",
    "x1 = tf.constant(10.)\n",
    "x2 = tf.constant(2.)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(x)\n",
    "    y = x + 2\n",
    "    y2 = y * x1\n",
    "    y3 = y2 * x2\n",
    "    \n",
    "print(tape.watched_variables())\n",
    "print(tape.gradient(y, x).numpy())\n",
    "print(tape.gradient(y2, x).numpy())\n",
    "print(tape.gradient(y3, x).numpy())\n",
    "del tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    g.watch(x)\n",
    "    y = x * x\n",
    "    z = y * y\n",
    "dz_dx = g.gradient(z, x)  # 108.0 (4*x^3 at x = 3)\n",
    "dy_dx = g.gradient(y, x)  # 6.0\n",
    "print(dz_dx)\n",
    "print(dy_dx)\n",
    "del g  # Drop the reference to the tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
